import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.decomposition import PCA
from sklearn.naive_bayes import GaussianNB
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier
from sklearn.model_selection import LeaveOneOut, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_score, recall_score
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
import logging
import datetime
import os
warnings.filterwarnings('ignore')

class ModelLogger:
    """æ¨¡å‹è®­ç»ƒå’Œé¢„æµ‹çš„æ—¥å¿—è®°å½•å™¨"""

    def __init__(self, log_file="model_training.log"):
        self.log_file = log_file
        self.setup_logger()

    def setup_logger(self):
        """è®¾ç½®æ—¥å¿—é…ç½®"""
        # åˆ›å»ºlogsç›®å½•
        if not os.path.exists('logs'):
            os.makedirs('logs')

        log_path = os.path.join('logs', self.log_file)

        # é…ç½®æ—¥å¿—æ ¼å¼
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_path, encoding='utf-8'),
                logging.StreamHandler()  # åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°
            ]
        )
        self.logger = logging.getLogger(__name__)

    def log_data_info(self, X, y, data_type="è®­ç»ƒ"):
        """è®°å½•æ•°æ®ä¿¡æ¯"""
        unique_labels, counts = np.unique(y, return_counts=True)
        self.logger.info(f"=== {data_type}æ•°æ®ä¿¡æ¯ ===")
        self.logger.info(f"æ ·æœ¬æ•°: {len(X)}, ç‰¹å¾æ•°: {X.shape[1]}")
        self.logger.info(f"ç±»åˆ«æ•°: {len(unique_labels)}")
        self.logger.info(f"ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique_labels, counts))}")

        # æ•°æ®è´¨é‡æ£€æŸ¥
        nan_count = np.isnan(X).sum()
        if nan_count > 0:
            self.logger.warning(f"å‘ç° {nan_count} ä¸ªNaNå€¼")

        # ç‰¹å¾ç»Ÿè®¡
        self.logger.info(f"ç‰¹å¾å€¼èŒƒå›´: {X.min():.6f} ~ {X.max():.6f}")
        self.logger.info(f"ç‰¹å¾å‡å€¼: {X.mean():.6f}, æ ‡å‡†å·®: {X.std():.6f}")

    def log_training_start(self, optimize_hyperparams):
        """è®°å½•è®­ç»ƒå¼€å§‹"""
        self.logger.info("=" * 50)
        self.logger.info("å¼€å§‹æ¨¡å‹è®­ç»ƒ")
        self.logger.info(f"æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        self.logger.info(f"è¶…å‚æ•°ä¼˜åŒ–: {'æ˜¯' if optimize_hyperparams else 'å¦'}")

    def log_preprocessing(self, original_shape, processed_shape):
        """è®°å½•é¢„å¤„ç†ä¿¡æ¯"""
        self.logger.info(f"é¢„å¤„ç†: {original_shape} -> {processed_shape}")
        reduction_ratio = (1 - processed_shape[1] / original_shape[1]) * 100
        self.logger.info(f"ç»´åº¦é™ä½: {reduction_ratio:.1f}%")

    def log_hyperparameter_optimization(self, param_type, best_params, best_score):
        """è®°å½•è¶…å‚æ•°ä¼˜åŒ–ç»“æœ"""
        self.logger.info(f"{param_type}ä¼˜åŒ–å®Œæˆ:")
        self.logger.info(f"  æœ€ä½³å‚æ•°: {best_params}")
        self.logger.info(f"  æœ€ä½³åˆ†æ•°: {best_score:.4f}")

    def log_training_complete(self, training_time):
        """è®°å½•è®­ç»ƒå®Œæˆ"""
        self.logger.info(f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")

    def log_prediction_start(self, n_samples):
        """è®°å½•é¢„æµ‹å¼€å§‹"""
        self.logger.info(f"å¼€å§‹é¢„æµ‹ {n_samples} ä¸ªæ ·æœ¬")

    def log_prediction_results(self, predictions, probabilities):
        """è®°å½•é¢„æµ‹ç»“æœ"""
        max_probs = probabilities.max(axis=1)
        self.logger.info(f"é¢„æµ‹å®Œæˆ:")
        self.logger.info(f"  å¹³å‡ç½®ä¿¡åº¦: {max_probs.mean():.3f}")
        self.logger.info(f"  æœ€é«˜ç½®ä¿¡åº¦: {max_probs.max():.3f}")
        self.logger.info(f"  æœ€ä½ç½®ä¿¡åº¦: {max_probs.min():.3f}")

        # é¢„æµ‹åˆ†å¸ƒ
        unique_preds, counts = np.unique(predictions, return_counts=True)
        pred_dist = dict(zip(unique_preds, counts))
        self.logger.info(f"  é¢„æµ‹åˆ†å¸ƒ: {pred_dist}")

    def log_evaluation_results(self, results):
        """è®°å½•è¯„ä¼°ç»“æœ"""
        self.logger.info("=== æ¨¡å‹è¯„ä¼°ç»“æœ ===")
        for metric, value in results.items():
            if isinstance(value, dict):
                self.logger.info(f"{metric}:")
                for k, v in value.items():
                    self.logger.info(f"  {k}: {v:.4f}")
            else:
                self.logger.info(f"{metric}: {value:.4f}")

    def log_error(self, error_msg, exception=None):
        """è®°å½•é”™è¯¯ä¿¡æ¯"""
        self.logger.error(f"é”™è¯¯: {error_msg}")
        if exception:
            self.logger.error(f"å¼‚å¸¸è¯¦æƒ…: {str(exception)}")

    def log_warning(self, warning_msg):
        """è®°å½•è­¦å‘Šä¿¡æ¯"""
        self.logger.warning(warning_msg)

class SmallSampleRegionClassifier:
    def __init__(self, enable_logging=True):
        self.scaler = StandardScaler()
        self.pca = PCA(n_components=10, whiten=True)
        self.label_encoder = LabelEncoder()
        self.models = self._init_models()
        self.ensemble = None
        self.use_simple_knn = True

        # åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨
        if enable_logging:
            self.logger = ModelLogger()
        else:
            self.logger = None
        self.feature_importance = None
        self.component_names = None

    def _init_models(self):
        return {
            'knn1': KNeighborsClassifier(n_neighbors=1, metric='euclidean'),
            'knn_cos': KNeighborsClassifier(n_neighbors=1, metric='cosine'),
            'knn3': KNeighborsClassifier(n_neighbors=3, metric='euclidean')
        }

    def _extract_advanced_features(self, X):
        # åŸºç¡€ç»Ÿè®¡ç‰¹å¾
        stats = np.column_stack([
            np.mean(X, axis=1),
            np.std(X, axis=1),
            np.max(X, axis=1),
            np.min(X, axis=1),
            np.median(X, axis=1)
        ])

        # æ¯”ä¾‹ç‰¹å¾ (ç›¸å¯¹å«é‡)
        X_sum = np.sum(X, axis=1, keepdims=True)
        X_ratio = X / (X_sum + 1e-8)


        ratios = []
        for i in range(min(10, X.shape[1])):
            for j in range(i+1, min(10, X.shape[1])):
                ratio = X[:, i] / (X[:, j] + 1e-8)
                ratios.append(ratio)
        ratio_features = np.column_stack(ratios) if ratios else np.empty((X.shape[0], 0))

        # åˆ†å¸ƒç‰¹å¾
        skewness = []
        kurtosis = []
        for i in range(X.shape[0]):
            row = X[i, :]
            skew = np.mean(((row - np.mean(row)) / (np.std(row) + 1e-8)) ** 3)
            kurt = np.mean(((row - np.mean(row)) / (np.std(row) + 1e-8)) ** 4) - 3
            skewness.append(skew)
            kurtosis.append(kurt)

        distribution_features = np.column_stack([skewness, kurtosis])

        return np.hstack([X, X_ratio, stats, ratio_features, distribution_features])

    def preprocess_data(self, X, y=None, fit=True):
        X_clean = np.nan_to_num(X, nan=0.0)

        if fit:
            X_features = self._extract_advanced_features(X_clean)
            X_scaled = self.scaler.fit_transform(X_features)
            X_pca = self.pca.fit_transform(X_scaled)
            if y is not None:
                y_encoded = self.label_encoder.fit_transform(y)
                return X_pca, y_encoded
            return X_pca
        else:
            X_features = self._extract_advanced_features(X_clean)
            X_scaled = self.scaler.transform(X_features)
            X_pca = self.pca.transform(X_scaled)
            return X_pca

    def train(self, X, y, optimize_hyperparams=True):
        """
        è®­ç»ƒæ¨¡å‹ï¼Œå¯é€‰æ‹©æ˜¯å¦ä¼˜åŒ–è¶…å‚æ•°

        Parameters:
        -----------
        X : array-like, shape = [n_samples, n_features]
            è®­ç»ƒç‰¹å¾
        y : array-like, shape = [n_samples]
            è®­ç»ƒæ ‡ç­¾
        optimize_hyperparams : bool, default=True
            æ˜¯å¦ä¼˜åŒ–è¶…å‚æ•°
        """
        import time
        start_time = time.time()

        # è®°å½•è®­ç»ƒå¼€å§‹
        if self.logger:
            self.logger.log_training_start(optimize_hyperparams)
            self.logger.log_data_info(X, y, "è®­ç»ƒ")

        n_samples = len(X)
        n_unique_labels = len(np.unique(y))

        # æ ·æœ¬æ•°æ£€æŸ¥å’Œè­¦å‘Š
        print(f"è®­ç»ƒæ•°æ®ä¿¡æ¯ï¼šæ ·æœ¬æ•°={n_samples}, ç±»åˆ«æ•°={n_unique_labels}")

        if n_samples < 10:
            warning_msg = "æ ·æœ¬æ•°è¿‡å°‘(< 10)ï¼Œæ¨¡å‹æ€§èƒ½å¯èƒ½ä¸ç¨³å®š"
            print(f"âš ï¸  è­¦å‘Šï¼š{warning_msg}")
            if self.logger:
                self.logger.log_warning(warning_msg)

        if n_samples == n_unique_labels:
            warning_msg = "æ¯ä¸ªç±»åˆ«åªæœ‰1ä¸ªæ ·æœ¬ï¼Œè¿™æ˜¯æå°æ ·æœ¬å­¦ä¹ é—®é¢˜"
            print(f"âš ï¸  è­¦å‘Šï¼š{warning_msg}")
            print("   å»ºè®®ï¼šè€ƒè™‘æ”¶é›†æ›´å¤šæ•°æ®æˆ–ä½¿ç”¨å…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•")
            if self.logger:
                self.logger.log_warning(warning_msg)

        if n_samples < 50:
            info_msg = "æ ·æœ¬æ•°è¾ƒå°‘ï¼Œå°†ä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯(LOOCV)è¿›è¡Œè¯„ä¼°"
            print(f"â„¹ï¸  æç¤ºï¼š{info_msg}")
            if self.logger:
                self.logger.logger.info(info_msg)

        # å¦‚æœé€‰æ‹©ä¼˜åŒ–è¶…å‚æ•°ï¼Œå…ˆè¿›è¡Œå‚æ•°ä¼˜åŒ–
        if optimize_hyperparams:
            print("\n=== å¼€å§‹è¶…å‚æ•°ä¼˜åŒ– ===")

            # ä¼˜åŒ–PCAç»„ä»¶æ•°
            print("1. ä¼˜åŒ–PCAç»„ä»¶æ•°...")
            pca_results = self.optimize_pca_components(X, y)
            print(f"   æœ€ä¼˜PCAç»„ä»¶æ•°: {pca_results['best_n_components']}")
            if self.logger:
                self.logger.log_hyperparameter_optimization(
                    "PCA",
                    {"n_components": pca_results['best_n_components']},
                    pca_results['best_score']
                )

            # ä¼˜åŒ–Kå€¼
            print("2. ä¼˜åŒ–Kå€¼...")
            k_results = self.optimize_k_values(X, y)
            print("   æœ€ä¼˜Kå€¼:")
            for metric, result in k_results.items():
                print(f"     {metric}: K={result['best_k']}, åˆ†æ•°={result['best_score']:.3f}")
                if self.logger:
                    self.logger.log_hyperparameter_optimization(
                        f"Kå€¼({metric})",
                        {"k": result['best_k']},
                        result['best_score']
                    )
        else:
            print("\n=== è·³è¿‡è¶…å‚æ•°ä¼˜åŒ–ï¼Œä½¿ç”¨é»˜è®¤å‚æ•° ===")

        # ä½¿ç”¨ä¼˜åŒ–åçš„å‚æ•°è¿›è¡Œæ•°æ®é¢„å¤„ç†
        original_shape = X.shape
        X_processed, y_processed = self.preprocess_data(X, y, fit=True)
        print(f"é¢„å¤„ç†åç‰¹å¾ç»´åº¦: {X_processed.shape}")

        if self.logger:
            self.logger.log_preprocessing(original_shape, X_processed.shape)

        # åˆ›å»ºä¼˜åŒ–åçš„æ¨¡å‹
        if hasattr(self, 'optimized_models') and self.optimized_models:
            # å¦‚æœå·²ç»æœ‰ä¼˜åŒ–çš„æ¨¡å‹ï¼Œä½¿ç”¨å®ƒä»¬
            print("ä½¿ç”¨ä¼˜åŒ–åçš„æ¨¡å‹å‚æ•°")
            self.knn_euclidean = self.optimized_models['knn_euclidean']
            self.knn_manhattan = self.optimized_models['knn_manhattan']
            self.knn_cosine = self.optimized_models['knn_cosine']
        else:
            # å¦åˆ™ä½¿ç”¨é»˜è®¤å€¼
            print("ä½¿ç”¨é»˜è®¤æ¨¡å‹å‚æ•°")
            self.knn_euclidean = KNeighborsClassifier(n_neighbors=1, metric='euclidean')
            self.knn_manhattan = KNeighborsClassifier(n_neighbors=1, metric='manhattan')
            self.knn_cosine = KNeighborsClassifier(n_neighbors=1, metric='cosine')

        # è®­ç»ƒæ¨¡å‹
        print("è®­ç»ƒæ¨¡å‹...")
        self.knn_euclidean.fit(X_processed, y_processed)
        self.knn_manhattan.fit(X_processed, y_processed)
        self.knn_cosine.fit(X_processed, y_processed)

        self.X_train = X_processed
        self.y_train = y_processed
        self.y_train_original = y  # ä¿å­˜åŸå§‹æ ‡ç­¾ç”¨äºé¢„æµ‹æ—¶çš„ç±»åˆ«æŸ¥æ‰¾

        # è®°å½•è®­ç»ƒå®Œæˆ
        training_time = time.time() - start_time
        print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆ")

        if self.logger:
            self.logger.log_training_complete(training_time)

        return self

    def _create_default_k_results(self):
        """åˆ›å»ºé»˜è®¤çš„Kå€¼ä¼˜åŒ–ç»“æœ"""
        return {
            'euclidean': {'best_k': 1, 'best_score': 0.0, 'all_results': {1: {'mean_score': 0.0, 'std_score': 0.0, 'scores': []}}},
            'manhattan': {'best_k': 1, 'best_score': 0.0, 'all_results': {1: {'mean_score': 0.0, 'std_score': 0.0, 'scores': []}}},
            'cosine': {'best_k': 1, 'best_score': 0.0, 'all_results': {1: {'mean_score': 0.0, 'std_score': 0.0, 'scores': []}}}
        }

    def optimize_k_values(self, X, y, k_range=None, cv_folds=3, scoring='f1_macro'):
        """
        ä½¿ç”¨äº¤å‰éªŒè¯ä¼˜åŒ–Kå€¼

        Parameters:
        -----------
        X : array-like, shape = [n_samples, n_features]
            è®­ç»ƒç‰¹å¾
        y : array-like, shape = [n_samples]
            è®­ç»ƒæ ‡ç­¾
        k_range : list, optional
            Kå€¼æœç´¢èŒƒå›´ï¼Œé»˜è®¤ä¸º1åˆ°min(5, n_samples-1)
        cv_folds : int, default=3
            äº¤å‰éªŒè¯æŠ˜æ•°
        scoring : str, default='f1_macro'
            è¯„ä¼°æŒ‡æ ‡

        Returns:
        --------
        dict : åŒ…å«ä¼˜åŒ–ç»“æœçš„å­—å…¸
        """
        X_processed, y_processed = self.preprocess_data(X, y, fit=True)
        n_samples = len(X_processed)

        if k_range is None:
            # å¯¹äº16åœ°åŒºÃ—3æ ·æœ¬çš„æ•°æ®é›†ï¼Œæ¯ç±»åªæœ‰3ä¸ªæ ·æœ¬ï¼ŒKå€¼åº”è¯¥é™åˆ¶ä¸º1-2
            n_unique_labels = len(np.unique(y))
            samples_per_class = n_samples // n_unique_labels
            if samples_per_class <= 3:
                # æ¯ç±»æ ·æœ¬æ•°è¾ƒå°‘æ—¶ï¼ŒKå€¼é™åˆ¶ä¸º1-2
                max_k = min(2, n_samples - 1)
            else:
                # åŸæœ‰é€»è¾‘
                max_k = min(5, n_samples - 1)
            k_range = list(range(1, max_k + 1))

        # è¿‡æ»¤æ‰è¶…è¿‡æ ·æœ¬æ•°çš„Kå€¼
        k_range = [k for k in k_range if k < n_samples]

        if not k_range:
            print(f"è­¦å‘Šï¼šæ ·æœ¬æ•°({n_samples})è¿‡å°‘ï¼Œæ— æ³•è¿›è¡ŒKå€¼ä¼˜åŒ–ï¼Œä½¿ç”¨é»˜è®¤K=1")
            return self._create_default_k_results()

        # æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        n_unique_labels = len(np.unique(y))
        samples_per_class = n_samples // n_unique_labels

        # å¯¹äºæå°æ ·æœ¬æˆ–æ¯ç±»æ ·æœ¬æ•°å¾ˆå°‘ï¼Œå¼ºåˆ¶ä½¿ç”¨LOOCV
        if n_samples <= 20 or samples_per_class < cv_folds:
            cv = LeaveOneOut()
            print(f"æ ·æœ¬æ•°({n_samples})è¾ƒå°‘æˆ–æ¯ç±»æ ·æœ¬æ•°({samples_per_class})ä¸è¶³ï¼Œä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯(LOOCV)")
        else:
            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)

        optimization_results = {}

        # æµ‹è¯•ä¸åŒè·ç¦»åº¦é‡çš„KNN
        metrics = ['euclidean', 'manhattan', 'cosine']

        for metric in metrics:
            metric_results = {}
            best_k = 1
            best_score = 0

            for k in k_range:
                if k >= len(X_processed):
                    continue

                knn = KNeighborsClassifier(n_neighbors=k, metric=metric)
                scores = cross_val_score(knn, X_processed, y_processed, cv=cv, scoring=scoring)

                mean_score = scores.mean()
                std_score = scores.std()

                metric_results[k] = {
                    'mean_score': mean_score,
                    'std_score': std_score,
                    'scores': scores
                }

                if mean_score > best_score:
                    best_score = mean_score
                    best_k = k

            optimization_results[metric] = {
                'best_k': best_k,
                'best_score': best_score,
                'all_results': metric_results
            }

        # æ›´æ–°æ¨¡å‹çš„Kå€¼
        self.optimized_models = {
            'knn_euclidean': KNeighborsClassifier(n_neighbors=optimization_results['euclidean']['best_k'], metric='euclidean'),
            'knn_manhattan': KNeighborsClassifier(n_neighbors=optimization_results['manhattan']['best_k'], metric='manhattan'),
            'knn_cosine': KNeighborsClassifier(n_neighbors=optimization_results['cosine']['best_k'], metric='cosine')
        }

        return optimization_results

    def evaluate_with_loocv(self, X, y):
        X_processed, y_processed = self.preprocess_data(X, y, fit=True)

        loo = LeaveOneOut()
        results = {}

        for name, model in self.models.items():
            scores = cross_val_score(model, X_processed, y_processed,
                                   cv=loo, scoring='f1_macro')
            results[name] = {
                'mean_f1': scores.mean(),
                'std_f1': scores.std(),
                'scores': scores
            }

        ensemble_scores = cross_val_score(self.ensemble, X_processed, y_processed,
                                        cv=loo, scoring='f1_macro')
        results['ensemble'] = {
            'mean_f1': ensemble_scores.mean(),
            'std_f1': ensemble_scores.std(),
            'scores': ensemble_scores
        }

        return results

    def evaluate_with_cv(self, X, y, cv_folds=3, scoring_metrics=['f1_macro', 'accuracy', 'precision_macro', 'recall_macro']):
        """
        ä½¿ç”¨k-foldäº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹

        Parameters:
        -----------
        X : array-like, shape = [n_samples, n_features]
            è®­ç»ƒç‰¹å¾
        y : array-like, shape = [n_samples]
            è®­ç»ƒæ ‡ç­¾
        cv_folds : int, default=3
            äº¤å‰éªŒè¯æŠ˜æ•°
        scoring_metrics : list, default=['f1_macro', 'accuracy', 'precision_macro', 'recall_macro']
            è¯„ä¼°æŒ‡æ ‡åˆ—è¡¨

        Returns:
        --------
        dict : åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸
        """
        X_processed, y_processed = self.preprocess_data(X, y, fit=True)
        n_samples = len(X_processed)

        # æ£€æŸ¥æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°
        n_unique_labels = len(np.unique(y))
        samples_per_class = n_samples // n_unique_labels

        # è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„äº¤å‰éªŒè¯ç­–ç•¥
        if n_samples <= 20 or samples_per_class < cv_folds:
            cv = LeaveOneOut()
            print(f"æ¨¡å‹è¯„ä¼°ï¼šæ ·æœ¬æ•°({n_samples})è¾ƒå°‘æˆ–æ¯ç±»æ ·æœ¬æ•°({samples_per_class})ä¸è¶³ï¼Œä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯")
        else:
            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
            print(f"æ¨¡å‹è¯„ä¼°ï¼šä½¿ç”¨{cv_folds}æŠ˜åˆ†å±‚äº¤å‰éªŒè¯")

        results = {}

        for name, model in self.models.items():
            model_results = {}

            for metric in scoring_metrics:
                try:
                    scores = cross_val_score(model, X_processed, y_processed, cv=cv, scoring=metric)
                    model_results[metric] = {
                        'mean': scores.mean(),
                        'std': scores.std(),
                        'scores': scores
                    }
                except Exception as e:
                    print(f"è¯„ä¼°æŒ‡æ ‡{metric}åœ¨æ¨¡å‹{name}ä¸Šå¤±è´¥: {e}")
                    model_results[metric] = {
                        'mean': 0.0,
                        'std': 0.0,
                        'scores': []
                    }

            results[name] = model_results

        return results

    def optimize_pca_components(self, X, y, n_components_range=None, cv_folds=3, scoring='f1_macro'):
        """
        ä¼˜åŒ–PCAç»„ä»¶æ•°

        Parameters:
        -----------
        X : array-like, shape = [n_samples, n_features]
            è®­ç»ƒç‰¹å¾
        y : array-like, shape = [n_samples]
            è®­ç»ƒæ ‡ç­¾
        n_components_range : list, optional
            PCAç»„ä»¶æ•°æœç´¢èŒƒå›´
        cv_folds : int, default=3
            äº¤å‰éªŒè¯æŠ˜æ•°
        scoring : str, default='f1_macro'
            è¯„ä¼°æŒ‡æ ‡

        Returns:
        --------
        dict : åŒ…å«ä¼˜åŒ–ç»“æœçš„å­—å…¸
        """
        X_clean = np.nan_to_num(X, nan=0.0)
        X_features = self._extract_advanced_features(X_clean)
        X_scaled = self.scaler.fit_transform(X_features)

        n_samples, n_features = X_scaled.shape

        if n_components_range is None:
            # å¯¹äº16åœ°åŒºÃ—3æ ·æœ¬çš„æ•°æ®é›†ï¼ˆ48æ ·æœ¬ï¼‰ï¼ŒPCAç»„ä»¶æ•°èŒƒå›´è°ƒæ•´ä¸º2-15
            n_unique_labels = len(np.unique(y))
            if n_unique_labels >= 10:  # å¯¹äºå¤šç±»åˆ«é—®é¢˜ï¼ˆå¦‚16ç±»ï¼‰
                max_components = min(min(n_samples, n_features) - 1, 15)
                n_components_range = list(range(2, max_components + 1))
            else:
                # åŸæœ‰é€»è¾‘ï¼šå¯¹äºç±»åˆ«è¾ƒå°‘çš„é—®é¢˜
                max_components = min(min(n_samples, n_features) - 1, 10)
                n_components_range = list(range(2, max_components + 1))

        # è¿‡æ»¤æ‰è¶…è¿‡æ ·æœ¬æ•°æˆ–ç‰¹å¾æ•°çš„ç»„ä»¶æ•°
        n_components_range = [n for n in n_components_range if n < min(n_samples, n_features)]

        if not n_components_range:
            print(f"è­¦å‘Šï¼šæ ·æœ¬æ•°({n_samples})æˆ–ç‰¹å¾æ•°({n_features})è¿‡å°‘ï¼Œæ— æ³•è¿›è¡ŒPCAä¼˜åŒ–ï¼Œä½¿ç”¨é»˜è®¤ç»„ä»¶æ•°")
            return {'best_n_components': min(5, min(n_samples, n_features) - 1), 'best_score': 0.0, 'all_results': {}}

        # å¯¹äºæå°æ ·æœ¬æˆ–æ¯ç±»æ ·æœ¬æ•°è¾ƒå°‘çš„æƒ…å†µï¼Œå¼ºåˆ¶ä½¿ç”¨LOOCV
        n_unique_labels = len(np.unique(y))
        samples_per_class = n_samples // n_unique_labels

        if n_samples <= 20 or samples_per_class <= 5:
            cv = LeaveOneOut()
            print(f"PCAä¼˜åŒ–ï¼šæ ·æœ¬æ•°({n_samples})è¾ƒå°‘æˆ–æ¯ç±»æ ·æœ¬æ•°({samples_per_class})è¾ƒå°‘ï¼Œä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯(LOOCV)")
        else:
            cv = StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42)
            print(f"PCAä¼˜åŒ–ï¼šä½¿ç”¨{cv_folds}æŠ˜åˆ†å±‚äº¤å‰éªŒè¯")

        y_encoded = self.label_encoder.fit_transform(y)

        pca_results = {}
        best_n_components = min(n_components_range) if n_components_range else 2
        best_score = 0

        print(f"PCAä¼˜åŒ–ï¼šæµ‹è¯•ç»„ä»¶æ•°èŒƒå›´ {n_components_range}")

        for n_comp in n_components_range:
            try:
                # åˆ›å»ºä¸´æ—¶PCA
                temp_pca = PCA(n_components=n_comp, whiten=True)
                X_pca = temp_pca.fit_transform(X_scaled)

                # æµ‹è¯•ä¸åŒçš„KNNæ¨¡å‹
                models_to_test = {
                    'euclidean': KNeighborsClassifier(n_neighbors=1, metric='euclidean'),
                    'manhattan': KNeighborsClassifier(n_neighbors=1, metric='manhattan'),
                    'cosine': KNeighborsClassifier(n_neighbors=1, metric='cosine')
                }

                component_scores = {}
                total_score = 0

                for metric_name, model in models_to_test.items():
                    scores = cross_val_score(model, X_pca, y_encoded, cv=cv, scoring=scoring)
                    component_scores[metric_name] = {
                        'mean': scores.mean(),
                        'std': scores.std(),
                        'scores': scores
                    }
                    total_score += scores.mean()

                avg_score = total_score / len(models_to_test)

                pca_results[n_comp] = {
                    'avg_score': avg_score,
                    'individual_scores': component_scores,
                    'variance_explained': temp_pca.explained_variance_ratio_.sum()
                }

                if avg_score > best_score:
                    best_score = avg_score
                    best_n_components = n_comp

            except Exception as e:
                print(f"PCAç»„ä»¶æ•°{n_comp}æµ‹è¯•å¤±è´¥: {e}")
                continue

        # æ›´æ–°PCAç»„ä»¶æ•°
        if best_n_components > 0:
            self.pca = PCA(n_components=best_n_components, whiten=True)
            print(f"PCAä¼˜åŒ–å®Œæˆï¼šæœ€ä¼˜ç»„ä»¶æ•° = {best_n_components}")

        return {
            'best_n_components': best_n_components,
            'best_score': best_score,
            'all_results': pca_results
        }

    def _calculate_adaptive_weights(self, distances_e, distances_m, distances_c):
        # åŸºäºè·ç¦»çš„è‡ªé€‚åº”æƒé‡
        inv_dist_e = 1.0 / (distances_e + 1e-8)
        inv_dist_m = 1.0 / (distances_m + 1e-8)
        inv_dist_c = 1.0 / (distances_c + 1e-8)

        total_inv = inv_dist_e + inv_dist_m + inv_dist_c
        weight_e = inv_dist_e / total_inv
        weight_m = inv_dist_m / total_inv
        weight_c = inv_dist_c / total_inv

        return weight_e, weight_m, weight_c

    def predict(self, X):
        # è®°å½•é¢„æµ‹å¼€å§‹
        if self.logger:
            self.logger.log_prediction_start(len(X))

        X_processed = self.preprocess_data(X, fit=False)

        pred_euclidean = self.knn_euclidean.predict(X_processed)
        pred_manhattan = self.knn_manhattan.predict(X_processed)
        pred_cosine = self.knn_cosine.predict(X_processed)

        distances_euclidean, _ = self.knn_euclidean.kneighbors(X_processed)
        distances_manhattan, _ = self.knn_manhattan.kneighbors(X_processed)
        distances_cosine, _ = self.knn_cosine.kneighbors(X_processed)

        final_predictions = []
        for i in range(len(X_processed)):
            dist_e = distances_euclidean[i][0]
            dist_m = distances_manhattan[i][0]
            dist_c = distances_cosine[i][0]

            weight_e, weight_m, weight_c = self._calculate_adaptive_weights(dist_e, dist_m, dist_c)

            # å¦‚æœæŸä¸ªè·ç¦»æ˜æ˜¾æœ€å°ï¼Œä½¿ç”¨è¯¥é¢„æµ‹
            if weight_e > 0.6:
                final_predictions.append(pred_euclidean[i])
            elif weight_m > 0.6:
                final_predictions.append(pred_manhattan[i])
            elif weight_c > 0.6:
                final_predictions.append(pred_cosine[i])
            else:
                # å¦åˆ™ä½¿ç”¨åŠ æƒæŠ•ç¥¨
                votes = [pred_euclidean[i], pred_manhattan[i], pred_cosine[i]]
                if len(set(votes)) == 1:
                    final_predictions.append(votes[0])
                else:
                    final_predictions.append(pred_euclidean[i])  # é»˜è®¤æ¬§æ°è·ç¦»

        predictions = self.label_encoder.inverse_transform(final_predictions)

        # è®°å½•é¢„æµ‹ç»“æœ
        if self.logger:
            probabilities = self.predict_proba(X)
            self.logger.log_prediction_results(predictions, probabilities)

        return predictions

    def predict_proba(self, X):
        X_processed = self.preprocess_data(X, fit=False)

        distances_euclidean, indices_euclidean = self.knn_euclidean.kneighbors(X_processed)
        distances_manhattan, indices_manhattan = self.knn_manhattan.kneighbors(X_processed)
        distances_cosine, indices_cosine = self.knn_cosine.kneighbors(X_processed)

        proba = np.zeros((len(X_processed), len(self.label_encoder.classes_)))

        for i in range(len(X_processed)):
            dist_e = distances_euclidean[i][0]
            dist_m = distances_manhattan[i][0]
            dist_c = distances_cosine[i][0]

            conf_e = max(0.5, 1.0 / (1.0 + dist_e * 2))
            conf_m = max(0.5, 1.0 / (1.0 + dist_m * 1.5))
            conf_c = max(0.5, 1.0 / (1.0 + dist_c * 3))

            avg_confidence = (conf_e + conf_m + conf_c) / 3
            final_confidence = min(0.95, max(0.6, avg_confidence))

            # è·å–æœ€è¿‘é‚»çš„æ ·æœ¬ç´¢å¼•
            sample_idx_e = indices_euclidean[i][0]

            # è¾¹ç•Œæ£€æŸ¥ï¼Œç¡®ä¿æ ·æœ¬ç´¢å¼•ä¸è¶Šç•Œ
            if sample_idx_e >= len(self.y_train):
                print(f"è­¦å‘Šï¼šæ ·æœ¬ç´¢å¼•{sample_idx_e}è¶…å‡ºè®­ç»ƒæ•°æ®èŒƒå›´ï¼Œè®­ç»ƒæ ·æœ¬æ•°ä¸º{len(self.y_train)}")
                sample_idx_e = 0  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ ·æœ¬ä½œä¸ºé»˜è®¤å€¼

            # è·å–è¯¥æ ·æœ¬å¯¹åº”çš„ç±»åˆ«ç´¢å¼•
            # ä½¿ç”¨åŸå§‹æ ‡ç­¾è€Œä¸æ˜¯ç¼–ç åçš„æ ‡ç­¾
            predicted_class = self.y_train_original[sample_idx_e]
            class_matches = np.where(self.label_encoder.classes_ == predicted_class)[0]

            if len(class_matches) == 0:
                print(f"è­¦å‘Šï¼šæœªæ‰¾åˆ°ç±»åˆ«'{predicted_class}'ï¼Œå¯ç”¨ç±»åˆ«: {self.label_encoder.classes_}")
                class_idx = 0  # ä½¿ç”¨ç¬¬ä¸€ä¸ªç±»åˆ«ä½œä¸ºé»˜è®¤å€¼
            else:
                class_idx = class_matches[0]

            proba[i, class_idx] = final_confidence
            remaining = (1.0 - final_confidence) / (len(self.label_encoder.classes_) - 1)
            proba[i, :] += remaining
            proba[i, class_idx] = final_confidence

        return proba

    def get_feature_importance(self):
        if self.pca is None:
            return None

        # PCAæˆåˆ†çš„é‡è¦æ€§
        pca_importance = np.abs(self.pca.components_).mean(axis=0)

        # å¦‚æœæœ‰æˆåˆ†åç§°ï¼Œè¿”å›å¸¦åç§°çš„é‡è¦æ€§
        if self.component_names is not None:
            importance_dict = {}
            for i, name in enumerate(self.component_names[:len(pca_importance)]):
                importance_dict[name] = pca_importance[i]
            return importance_dict

        return pca_importance

    def explain_prediction(self, X, sample_idx=0):
        X_processed = self.preprocess_data(X, fit=False)

        distances_e, indices_e = self.knn_euclidean.kneighbors([X_processed[sample_idx]])
        distances_m, indices_m = self.knn_manhattan.kneighbors([X_processed[sample_idx]])
        distances_c, indices_c = self.knn_cosine.kneighbors([X_processed[sample_idx]])

        explanation = {
            'euclidean_distance': distances_e[0][0],
            'manhattan_distance': distances_m[0][0],
            'cosine_distance': distances_c[0][0],
            'nearest_region_euclidean': self.label_encoder.inverse_transform([self.y_train[indices_e[0][0]]])[0],
            'nearest_region_manhattan': self.label_encoder.inverse_transform([self.y_train[indices_m[0][0]]])[0],
            'nearest_region_cosine': self.label_encoder.inverse_transform([self.y_train[indices_c[0][0]]])[0],
        }

        weight_e, weight_m, weight_c = self._calculate_adaptive_weights(
            distances_e[0][0], distances_m[0][0], distances_c[0][0]
        )
        explanation['distance_weights'] = {
            'euclidean': weight_e,
            'manhattan': weight_m,
            'cosine': weight_c
        }

        return explanation

    def generate_optimization_report(self, X, y, save_to_file=False, filename='optimization_report.txt'):
        """
        ç”Ÿæˆè¶…å‚æ•°ä¼˜åŒ–æŠ¥å‘Š

        Parameters:
        -----------
        X : array-like, shape = [n_samples, n_features]
            è®­ç»ƒç‰¹å¾
        y : array-like, shape = [n_samples]
            è®­ç»ƒæ ‡ç­¾
        save_to_file : bool, default=False
            æ˜¯å¦ä¿å­˜åˆ°æ–‡ä»¶
        filename : str, default='optimization_report.txt'
            æŠ¥å‘Šæ–‡ä»¶å

        Returns:
        --------
        str : ä¼˜åŒ–æŠ¥å‘Šå†…å®¹
        """
        n_samples = len(X)
        n_unique_labels = len(np.unique(y))

        report = []
        report.append("=" * 60)
        report.append("è¯æåœ°åŒºåˆ†ç±»å™¨ - è¶…å‚æ•°ä¼˜åŒ–æŠ¥å‘Š")
        report.append("=" * 60)

        # æ•°æ®é›†ä¿¡æ¯
        report.append(f"\nğŸ“Š æ•°æ®é›†ä¿¡æ¯:")
        report.append(f"   æ ·æœ¬æ•°: {n_samples}")
        report.append(f"   ç±»åˆ«æ•°: {n_unique_labels}")
        report.append(f"   ç‰¹å¾æ•°: {X.shape[1]}")

        # æ ·æœ¬é‡åˆ†æ
        if n_samples == n_unique_labels:
            report.append(f"\nâš ï¸  æå°æ ·æœ¬è­¦å‘Š:")
            report.append(f"   æ¯ä¸ªç±»åˆ«åªæœ‰1ä¸ªæ ·æœ¬ï¼Œè¿™æ˜¯æå°æ ·æœ¬å­¦ä¹ é—®é¢˜")
            report.append(f"   å»ºè®®ï¼šè€ƒè™‘æ”¶é›†æ›´å¤šæ•°æ®æˆ–ä½¿ç”¨å…¶ä»–æœºå™¨å­¦ä¹ æ–¹æ³•")
        elif n_samples < 50:
            report.append(f"\nâš ï¸  å°æ ·æœ¬æç¤º:")
            report.append(f"   æ ·æœ¬æ•°è¾ƒå°‘ï¼Œä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯(LOOCV)è¿›è¡Œè¯„ä¼°")

        # äº¤å‰éªŒè¯ç­–ç•¥
        report.append(f"\nğŸ” äº¤å‰éªŒè¯ç­–ç•¥:")
        if n_samples <= 20:
            report.append(f"   ä½¿ç”¨ç•™ä¸€æ³•äº¤å‰éªŒè¯(LOOCV) - é€‚åˆæå°æ ·æœ¬")
        else:
            report.append(f"   ä½¿ç”¨5æŠ˜åˆ†å±‚äº¤å‰éªŒè¯")

        # PCAç»„ä»¶æ•°ä¼˜åŒ–
        report.append("\n1. PCAç»„ä»¶æ•°ä¼˜åŒ–ç»“æœ:")
        report.append("-" * 40)

        try:
            pca_results = self.optimize_pca_components(X, y)
            report.append(f"   æœ€ä¼˜PCAç»„ä»¶æ•°: {pca_results['best_n_components']}")
            report.append(f"   æœ€ä¼˜åˆ†æ•°: {pca_results['best_score']:.4f}")

            if pca_results['all_results']:
                report.append("\n   è¯¦ç»†ç»“æœ:")
                for n_comp, result in list(pca_results['all_results'].items())[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                    report.append(f"     ç»„ä»¶æ•°{n_comp}: å¹³å‡åˆ†æ•°={result['avg_score']:.4f}, æ–¹å·®è§£é‡Šç‡={result['variance_explained']:.4f}")
        except Exception as e:
            report.append(f"   PCAä¼˜åŒ–å¤±è´¥: {e}")

        # Kå€¼ä¼˜åŒ–
        report.append("\n2. Kå€¼ä¼˜åŒ–ç»“æœ:")
        report.append("-" * 40)

        try:
            k_results = self.optimize_k_values(X, y)
            for metric, result in k_results.items():
                report.append(f"\n   {metric}è·ç¦»:")
                report.append(f"     æœ€ä¼˜Kå€¼: {result['best_k']}")
                report.append(f"     æœ€ä¼˜åˆ†æ•°: {result['best_score']:.4f}")

                if result['all_results']:
                    report.append("     è¯¦ç»†ç»“æœ:")
                    for k, k_result in list(result['all_results'].items())[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                        report.append(f"       K={k}: {k_result['mean_score']:.4f}Â±{k_result['std_score']:.4f}")
        except Exception as e:
            report.append(f"   Kå€¼ä¼˜åŒ–å¤±è´¥: {e}")

        # äº¤å‰éªŒè¯ç»“æœ
        report.append("\n3. äº¤å‰éªŒè¯è¯„ä¼°ç»“æœ:")
        report.append("-" * 40)

        try:
            cv_results = self.evaluate_with_cv(X, y)
            for model_name, metrics in cv_results.items():
                report.append(f"\n   {model_name}:")
                for metric_name, metric_result in metrics.items():
                    if metric_result['mean'] > 0:  # åªæ˜¾ç¤ºæˆåŠŸçš„æŒ‡æ ‡
                        report.append(f"     {metric_name}: {metric_result['mean']:.4f}Â±{metric_result['std']:.4f}")
        except Exception as e:
            report.append(f"   äº¤å‰éªŒè¯è¯„ä¼°å¤±è´¥: {e}")

        # å»ºè®®å’Œæ€»ç»“
        report.append("\n4. ä¼˜åŒ–å»ºè®®å’Œæ€»ç»“:")
        report.append("-" * 40)

        try:
            # åˆ†æç»“æœå¹¶ç»™å‡ºå»ºè®®
            if 'k_results' in locals():
                best_model = max(k_results.items(), key=lambda x: x[1]['best_score'])
                report.append(f"   ğŸ¯ æ¨èé…ç½®:")
                report.append(f"     è·ç¦»åº¦é‡: {best_model[0]}")
                report.append(f"     Kå€¼: {best_model[1]['best_k']}")

                if 'pca_results' in locals():
                    report.append(f"     PCAç»„ä»¶æ•°: {pca_results['best_n_components']}")

            # æå°æ ·æœ¬ç‰¹æ®Šå»ºè®®
            if n_samples <= 20:
                report.append(f"\n   ğŸ’¡ æå°æ ·æœ¬å»ºè®®:")
                report.append(f"     - å½“å‰æ ·æœ¬æ•°({n_samples})è¾ƒå°‘ï¼Œæ¨¡å‹æ³›åŒ–èƒ½åŠ›æœ‰é™")
                report.append(f"     - å»ºè®®æ”¶é›†æ›´å¤šè®­ç»ƒæ•°æ®æé«˜æ¨¡å‹ç¨³å®šæ€§")
                report.append(f"     - å¯è€ƒè™‘ä½¿ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯æ‰©å……æ ·æœ¬")
                report.append(f"     - æˆ–å°è¯•è¿ç§»å­¦ä¹ ã€å…ƒå­¦ä¹ ç­‰æ–¹æ³•")

        except Exception as e:
            report.append(f"   å»ºè®®ç”Ÿæˆå¤±è´¥: {e}")

        report.append(f"\n" + "=" * 60)
        report.append(f"æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}")

        report_text = "\n".join(report)

        if save_to_file:
            with open(filename, 'w', encoding='utf-8') as f:
                f.write(report_text)
            print(f"ğŸ“„ ä¼˜åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")

        return report_text

def load_data(train_file_path, test_file_path=None):
    """
    åŠ è½½è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®

    Parameters:
    -----------
    train_file_path : str
        è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„
    test_file_path : str, optional
        æµ‹è¯•æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™åªåŠ è½½è®­ç»ƒæ•°æ®

    Returns:
    --------
    tuple : (X_train, y_train, X_test, test_regions, test_samples) æˆ– (X_train, y_train)
    """
    # åŠ è½½è®­ç»ƒæ•°æ®
    train_data = pd.read_excel(train_file_path, sheet_name=0, header=None)

    # æå–ç‰¹å¾æ•°æ®ï¼ˆä»ç¬¬3è¡Œå¼€å§‹ï¼Œç¬¬2åˆ—åˆ°ç¬¬41åˆ—ï¼Œå…±40ä¸ªç‰¹å¾ï¼‰
    X_train = train_data.iloc[2:, 1:41].values.astype(float)

    # æå–åœ°åŒºæ ‡ç­¾ï¼ˆç¬¬1åˆ—ï¼‰ï¼Œå¹¶æå–åœ°åŒºä»£ç 
    raw_labels = train_data.iloc[2:, 0].values
    y_train = []
    for label in raw_labels:
        if isinstance(label, str) and '-' in label:
            # æå–åœ°åŒºä»£ç  (å¦‚ Y-FJ-1 -> Y-FJ)
            parts = label.split('-')
            if len(parts) >= 3:
                region_code = '-'.join(parts[:2])  # å–å‰ä¸¤éƒ¨åˆ†ä½œä¸ºåœ°åŒºä»£ç 
            else:
                region_code = label
        else:
            region_code = 'Unknown'
        y_train.append(region_code)

    y_train = np.array(y_train)

    print(f"è®­ç»ƒæ•°æ®åŠ è½½å®Œæˆ:")
    print(f"  æ ·æœ¬æ•°: {X_train.shape[0]}")
    print(f"  ç‰¹å¾æ•°: {X_train.shape[1]}")
    print(f"  åœ°åŒºæ•°: {len(np.unique(y_train))}")
    print(f"  åœ°åŒºåˆ—è¡¨: {list(np.unique(y_train))}")

    # å¦‚æœæ²¡æœ‰æä¾›æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼Œåªè¿”å›è®­ç»ƒæ•°æ®
    if test_file_path is None:
        return X_train, y_train

    # åŠ è½½æµ‹è¯•æ•°æ®
    try:
        test_data = pd.read_excel(test_file_path, sheet_name=0, header=None)

        # æµ‹è¯•é›†æ ·æœ¬IDï¼ˆç¬¬1åˆ—ï¼Œä»ç¬¬3è¡Œå¼€å§‹ï¼‰
        test_sample_ids = test_data.iloc[2:, 0].dropna().tolist()

        # æµ‹è¯•é›†åœ°åŒºæ ‡ç­¾ï¼ˆç¬¬2åˆ—ï¼Œä»ç¬¬3è¡Œå¼€å§‹ï¼‰
        test_region_labels = test_data.iloc[2:, 1].dropna().tolist()

        # ä»åœ°åŒºæ ‡ç­¾ä¸­æå–åœ°åŒºä»£ç 
        test_regions = []
        for region_label in test_region_labels:
            if isinstance(region_label, str) and '-' in region_label:
                # æå–åœ°åŒºä»£ç  (å¦‚ "Y-FJ aver" -> "Y-FJ")
                parts = region_label.strip().replace(' aver', '').split('-')
                if len(parts) >= 2:
                    region_code = '-'.join(parts[:2])
                else:
                    region_code = parts[0]
            else:
                region_code = 'Unknown'
            test_regions.append(region_code)

        # æµ‹è¯•é›†ç‰¹å¾æ•°æ®ï¼ˆç¬¬3åˆ—å¼€å§‹ï¼Œä»ç¬¬3è¡Œå¼€å§‹ï¼Œå–40ä¸ªç‰¹å¾ä»¥åŒ¹é…è®­ç»ƒé›†ï¼‰
        X_test = test_data.iloc[2:, 2:42].values.astype(float)

        print(f"æµ‹è¯•æ•°æ®åŠ è½½å®Œæˆ:")
        print(f"  æµ‹è¯•æ ·æœ¬æ•°: {X_test.shape[0]}")
        print(f"  ç‰¹å¾æ•°: {X_test.shape[1]}")
        print(f"  åœ°åŒºæ•°: {len(set(test_regions))}")
        print(f"  åœ°åŒºåˆ—è¡¨: {list(set(test_regions))}")
        print(f"  æ ·æœ¬ID: {test_sample_ids}")

        return X_train, y_train, X_test, test_regions, test_sample_ids

    except Exception as e:
        print(f"æµ‹è¯•æ•°æ®åŠ è½½å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return X_train, y_train, np.empty((0, 40)), [], []

def load_data_legacy(file_path):
    """
    å…¼å®¹æ—§ç‰ˆæœ¬çš„æ•°æ®åŠ è½½å‡½æ•°ï¼ˆç”¨äºæ—§æ ¼å¼æ•°æ®ï¼‰
    """
    train_data = pd.read_excel(file_path, sheet_name=0)
    test_data = pd.read_excel(file_path, sheet_name=1)

    X_train = train_data.iloc[1:, 2:40].values.astype(float)
    y_train = train_data.iloc[1:, 1].values

    test_samples = []
    test_regions = []

    for col_idx in range(2, test_data.shape[1]):
        col_name = test_data.columns[col_idx]
        if 'aver' not in col_name:
            try:
                sample = pd.to_numeric(test_data.iloc[:38, col_idx], errors='coerce').values
                if not np.isnan(sample).all() and len(sample) == 38:
                    test_samples.append(sample)
                    region_parts = col_name.split('-')
                    if len(region_parts) >= 2:
                        region_name = f"{region_parts[0]}-{region_parts[1]}"
                    else:
                        region_name = col_name
                    test_regions.append(region_name)
            except:
                continue

    X_test = np.array(test_samples)

    return X_train, y_train, X_test, test_regions

def bootstrap_evaluation(classifier, X, y, n_bootstrap=100):
    n_samples = len(X)
    f1_scores = []

    for _ in range(n_bootstrap):
        indices = np.random.choice(n_samples, n_samples, replace=True)
        X_boot = X[indices]
        y_boot = y[indices]

        classifier_boot = SmallSampleRegionClassifier()
        classifier_boot.train(X_boot, y_boot)

        oob_indices = np.setdiff1d(np.arange(n_samples), indices)
        if len(oob_indices) > 0:
            X_oob = X[oob_indices]
            y_oob = y[oob_indices]
            y_pred = classifier_boot.predict(X_oob)
            f1 = f1_score(y_oob, y_pred, average='macro')
            f1_scores.append(f1)

    return np.array(f1_scores)

def main():
    # ä½¿ç”¨æ–°çš„æ•°æ®åŠ è½½å‡½æ•°
    X_train, y_train = load_data('è®­ç»ƒé›†.xlsx')

    print(f"è®­ç»ƒæ•°æ®: {X_train.shape}, åœ°åŒºæ•°: {len(np.unique(y_train))}")

    classifier = SmallSampleRegionClassifier()

    # è®­ç»ƒæ¨¡å‹ï¼ˆåŒ…å«è¶…å‚æ•°ä¼˜åŒ–ï¼‰
    print("\n=== å¼€å§‹è®­ç»ƒå’Œä¼˜åŒ– ===")
    classifier.train(X_train, y_train, optimize_hyperparams=True)

    # ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š
    print("\n=== ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š ===")
    report = classifier.generate_optimization_report(X_train, y_train, save_to_file=True)
    print("\n" + report)

    # ä½¿ç”¨æ–°çš„äº¤å‰éªŒè¯æ–¹æ³•è¯„ä¼°
    print("\n=== æ ‡å‡†äº¤å‰éªŒè¯è¯„ä¼°ç»“æœ ===")
    cv_results = classifier.evaluate_with_cv(X_train, y_train)
    for model_name, metrics in cv_results.items():
        print(f"\n{model_name}:")
        for metric_name, metric_result in metrics.items():
            print(f"  {metric_name}: {metric_result['mean']:.3f}Â±{metric_result['std']:.3f}")

    # LOOCVè¯„ä¼°ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
    print("\n=== LOOCVè¯„ä¼°ç»“æœ ===")
    loocv_results = classifier.evaluate_with_loocv(X_train, y_train)
    for model_name, metrics in loocv_results.items():
        print(f"{model_name}: F1={metrics['mean_f1']:.3f}Â±{metrics['std_f1']:.3f}")

    # Bootstrapè¯„ä¼°ï¼ˆä¿ç•™åŸæœ‰åŠŸèƒ½ï¼‰
    print("\n=== Bootstrapè¯„ä¼°ç»“æœ ===")
    bootstrap_scores = bootstrap_evaluation(classifier, X_train, y_train)
    print(f"Bootstrap F1: {bootstrap_scores.mean():.3f}Â±{bootstrap_scores.std():.3f}")
    print(f"95%ç½®ä¿¡åŒºé—´: [{np.percentile(bootstrap_scores, 2.5):.3f}, {np.percentile(bootstrap_scores, 97.5):.3f}]")

    return classifier

if __name__ == "__main__":
    classifier = main()
